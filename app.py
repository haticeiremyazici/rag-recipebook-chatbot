# -----------------------------------------------------------
# APP.PY NÄ°HAÄ° VE HATASIZ VERSÄ°YON
# -----------------------------------------------------------
import streamlit as st
import os
from streamlit_chromadb_connection.chromadb_connection import ChromadbConnection # <<< YENÄ° BAÄžLANTI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma # <<< CHROMA MODÃœLÃœNÃœN STABÄ°L YOLU
from streamlit import secrets 

# -----------------------------------------------------------
# 1. BAÄžLANTI VE RAG SÄ°STEMÄ° KURULUMU
# -----------------------------------------------------------

# API Key'i Streamlit Secrets'tan okuma
try:
    GEMINI_KEY = secrets["GEMINI_API_KEY"]
except:
    GEMINI_KEY = os.getenv("GEMINI_API_KEY") 

# Embedding Modeli (HuggingFace, kota sorununu aÅŸan model)
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# ChromaDB'ye baÄŸlanma (st.connection kullanÄ±larak)
# path: Colab'de oluÅŸturduÄŸumuz ve GitHub'a yÃ¼klediÄŸimiz klasÃ¶rÃ¼n yolu.
conn = st.connection("chromadb", type=ChromadbConnection, path="./chroma_db", embedding_function=embeddings)

# Chroma'dan LangChain Retriever'Ä± Ã§ekme
# Not: st.connection'Ä±n dÃ¶ndÃ¼rdÃ¼ÄŸÃ¼ client'Ä± LangChain'e tanÄ±tÄ±yoruz.
vector_store = Chroma(client=conn.get_client(), collection_name="langchain", embedding_function=embeddings)
retriever = vector_store.as_retriever(search_kwargs={"k": 3})

# LLM Modelini TanÄ±mla
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.2, api_key=GEMINI_KEY)

# Prompt ve LCEL Zinciri (AynÄ± kalÄ±r)
prompt_template = """
Sen, sadece saÄŸlanan tariflere dayanarak cevap veren bir Yemek Tarifi UzmanÄ±sÄ±n. [...]
BaÄŸlam (Tarif): {context}
Soru: {question}
Cevap:
"""
prompt = ChatPromptTemplate.from_template(prompt_template)

rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# -----------------------------------------------------------
# STREAMLIT ARAYÃœZ KODU
# -----------------------------------------------------------
st.title("ðŸ° Tarif Defteri AsistanÄ± Chatbot (RAG)")
st.write("PDF'teki tariflere dayanarak cevap veriyorum.")

query = st.text_input("Hangi tatlÄ±nÄ±n malzemelerini veya tarifini Ã¶ÄŸrenmek istersiniz?")

if query:
    with st.spinner('Tarif defterinde arama yapÄ±lÄ±yor...'):
        response = rag_chain.invoke(query)
        st.success("Tarif CevabÄ±:")
        st.info(response)
